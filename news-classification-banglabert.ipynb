{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":8926716,"sourceType":"datasetVersion","datasetId":5358797}],"dockerImageVersionId":30733,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import pandas as pd ","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-07-11T03:27:39.553788Z","iopub.execute_input":"2024-07-11T03:27:39.554482Z","iopub.status.idle":"2024-07-11T03:27:39.921303Z","shell.execute_reply.started":"2024-07-11T03:27:39.554449Z","shell.execute_reply":"2024-07-11T03:27:39.920420Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"df = pd.read_csv('/kaggle/input/news-classification/bangla_news_classification.csv')","metadata":{"execution":{"iopub.status.busy":"2024-07-11T03:27:41.004679Z","iopub.execute_input":"2024-07-11T03:27:41.005570Z","iopub.status.idle":"2024-07-11T03:27:41.184667Z","shell.execute_reply.started":"2024-07-11T03:27:41.005538Z","shell.execute_reply":"2024-07-11T03:27:41.183880Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"df","metadata":{"execution":{"iopub.status.busy":"2024-07-11T03:27:43.428866Z","iopub.execute_input":"2024-07-11T03:27:43.429667Z","iopub.status.idle":"2024-07-11T03:27:43.449894Z","shell.execute_reply.started":"2024-07-11T03:27:43.429630Z","shell.execute_reply":"2024-07-11T03:27:43.449035Z"},"trusted":true},"execution_count":3,"outputs":[{"execution_count":3,"output_type":"execute_result","data":{"text/plain":"                                                headline  \\\n0      পুরুষদের আত্মহত্যা বাড়ার জন্য নারীরা দায়ী: দক্...   \n1         ফোন কলে আড়িপাতার অনুমতি পেল পাকিস্তানের আইএসআই   \n2      পাকিস্তানে পৃথক হামলায় নিরাপত্তা বাহিনীর ৫ সদস...   \n3      ভারতে দ্বিতল বাসের সঙ্গে দুধ ভর্তি ট্যাংকারের ...   \n4       ওয়াশিংটনে নেটোর সম্মেলনে জোরালো বক্তব্য বাইডেনের   \n...                                                  ...   \n43882  নিজেদের লোক দিয়েই নির্বাচন কমিশন গঠন করেছে আ. ...   \n43883  আমলে না নিলেও বিএনপিকে পরামর্শ দেবেন ডা. জাফরু...   \n43884  অগ্নিঝরা মার্চের প্রথম প্রহরে বঙ্গবন্ধুর প্রতি...   \n43885  দ্রব্যমূল্য নিয়ন্ত্রণে ব্যর্থ হলে সরকারকে পদত্...   \n43886  বিএনপি নির্বাচনের জন্য অযোগ্য হয়ে পড়েছে: ওবায়দ...   \n\n                     category  \n0      international_politics  \n1      international_politics  \n2      international_politics  \n3      international_politics  \n4      international_politics  \n...                       ...  \n43882             bd_politics  \n43883             bd_politics  \n43884             bd_politics  \n43885             bd_politics  \n43886             bd_politics  \n\n[43887 rows x 2 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>headline</th>\n      <th>category</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>পুরুষদের আত্মহত্যা বাড়ার জন্য নারীরা দায়ী: দক্...</td>\n      <td>international_politics</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>ফোন কলে আড়িপাতার অনুমতি পেল পাকিস্তানের আইএসআই</td>\n      <td>international_politics</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>পাকিস্তানে পৃথক হামলায় নিরাপত্তা বাহিনীর ৫ সদস...</td>\n      <td>international_politics</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>ভারতে দ্বিতল বাসের সঙ্গে দুধ ভর্তি ট্যাংকারের ...</td>\n      <td>international_politics</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>ওয়াশিংটনে নেটোর সম্মেলনে জোরালো বক্তব্য বাইডেনের</td>\n      <td>international_politics</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>43882</th>\n      <td>নিজেদের লোক দিয়েই নির্বাচন কমিশন গঠন করেছে আ. ...</td>\n      <td>bd_politics</td>\n    </tr>\n    <tr>\n      <th>43883</th>\n      <td>আমলে না নিলেও বিএনপিকে পরামর্শ দেবেন ডা. জাফরু...</td>\n      <td>bd_politics</td>\n    </tr>\n    <tr>\n      <th>43884</th>\n      <td>অগ্নিঝরা মার্চের প্রথম প্রহরে বঙ্গবন্ধুর প্রতি...</td>\n      <td>bd_politics</td>\n    </tr>\n    <tr>\n      <th>43885</th>\n      <td>দ্রব্যমূল্য নিয়ন্ত্রণে ব্যর্থ হলে সরকারকে পদত্...</td>\n      <td>bd_politics</td>\n    </tr>\n    <tr>\n      <th>43886</th>\n      <td>বিএনপি নির্বাচনের জন্য অযোগ্য হয়ে পড়েছে: ওবায়দ...</td>\n      <td>bd_politics</td>\n    </tr>\n  </tbody>\n</table>\n<p>43887 rows × 2 columns</p>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"category_counts = df['category'].value_counts()\nprint(category_counts)","metadata":{"execution":{"iopub.status.busy":"2024-07-11T03:27:52.079302Z","iopub.execute_input":"2024-07-11T03:27:52.079634Z","iopub.status.idle":"2024-07-11T03:27:52.097993Z","shell.execute_reply.started":"2024-07-11T03:27:52.079609Z","shell.execute_reply":"2024-07-11T03:27:52.097001Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"category\nbd_politics               16379\nnon_politics              14806\ninternational_politics    12702\nName: count, dtype: int64\n","output_type":"stream"}]},{"cell_type":"code","source":"import re\n\n# Function to clean text\ndef clean_text(text):\n    if isinstance(text, str):\n        # Remove special characters and extra spaces\n        text = text.replace('‘', '').replace('’', '').replace('“', '').replace('”', '').strip()\n        # Remove Bengali numbers\n        text = re.sub(r'[০-৯]', '', text)\n    return text\n\n# Apply text cleaning\ndf['headline'] = df['headline'].apply(clean_text)\n\n# Remove duplicates\ndf.drop_duplicates(subset='headline', keep='first', inplace=True)\n\n# Handle null values (if any)\ndf.dropna(subset=['headline', 'category'], inplace=True)","metadata":{"execution":{"iopub.status.busy":"2024-07-11T03:28:32.199566Z","iopub.execute_input":"2024-07-11T03:28:32.200182Z","iopub.status.idle":"2024-07-11T03:28:32.368653Z","shell.execute_reply.started":"2024-07-11T03:28:32.200150Z","shell.execute_reply":"2024-07-11T03:28:32.367885Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"category_counts = df['category'].value_counts()\nprint(category_counts)","metadata":{"execution":{"iopub.status.busy":"2024-07-11T03:28:38.446426Z","iopub.execute_input":"2024-07-11T03:28:38.447126Z","iopub.status.idle":"2024-07-11T03:28:38.457235Z","shell.execute_reply.started":"2024-07-11T03:28:38.447094Z","shell.execute_reply":"2024-07-11T03:28:38.456309Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stdout","text":"category\nnon_politics              14416\nbd_politics               12503\ninternational_politics     8909\nName: count, dtype: int64\n","output_type":"stream"}]},{"cell_type":"code","source":"df.reset_index(drop=True, inplace=True)\n","metadata":{"execution":{"iopub.status.busy":"2024-07-11T03:28:41.382427Z","iopub.execute_input":"2024-07-11T03:28:41.382802Z","iopub.status.idle":"2024-07-11T03:28:41.387357Z","shell.execute_reply.started":"2024-07-11T03:28:41.382775Z","shell.execute_reply":"2024-07-11T03:28:41.386367Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"df","metadata":{"execution":{"iopub.status.busy":"2024-07-11T03:28:42.102875Z","iopub.execute_input":"2024-07-11T03:28:42.103641Z","iopub.status.idle":"2024-07-11T03:28:42.113849Z","shell.execute_reply.started":"2024-07-11T03:28:42.103612Z","shell.execute_reply":"2024-07-11T03:28:42.112886Z"},"trusted":true},"execution_count":9,"outputs":[{"execution_count":9,"output_type":"execute_result","data":{"text/plain":"                                                headline  \\\n0      পুরুষদের আত্মহত্যা বাড়ার জন্য নারীরা দায়ী: দক্...   \n1         ফোন কলে আড়িপাতার অনুমতি পেল পাকিস্তানের আইএসআই   \n2      পাকিস্তানে পৃথক হামলায় নিরাপত্তা বাহিনীর  সদস্...   \n3      ভারতে দ্বিতল বাসের সঙ্গে দুধ ভর্তি ট্যাংকারের ...   \n4       ওয়াশিংটনে নেটোর সম্মেলনে জোরালো বক্তব্য বাইডেনের   \n...                                                  ...   \n35823  নিজেদের লোক দিয়েই নির্বাচন কমিশন গঠন করেছে আ. ...   \n35824  আমলে না নিলেও বিএনপিকে পরামর্শ দেবেন ডা. জাফরু...   \n35825  অগ্নিঝরা মার্চের প্রথম প্রহরে বঙ্গবন্ধুর প্রতি...   \n35826  দ্রব্যমূল্য নিয়ন্ত্রণে ব্যর্থ হলে সরকারকে পদত্...   \n35827  বিএনপি নির্বাচনের জন্য অযোগ্য হয়ে পড়েছে: ওবায়দ...   \n\n                     category  \n0      international_politics  \n1      international_politics  \n2      international_politics  \n3      international_politics  \n4      international_politics  \n...                       ...  \n35823             bd_politics  \n35824             bd_politics  \n35825             bd_politics  \n35826             bd_politics  \n35827             bd_politics  \n\n[35828 rows x 2 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>headline</th>\n      <th>category</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>পুরুষদের আত্মহত্যা বাড়ার জন্য নারীরা দায়ী: দক্...</td>\n      <td>international_politics</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>ফোন কলে আড়িপাতার অনুমতি পেল পাকিস্তানের আইএসআই</td>\n      <td>international_politics</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>পাকিস্তানে পৃথক হামলায় নিরাপত্তা বাহিনীর  সদস্...</td>\n      <td>international_politics</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>ভারতে দ্বিতল বাসের সঙ্গে দুধ ভর্তি ট্যাংকারের ...</td>\n      <td>international_politics</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>ওয়াশিংটনে নেটোর সম্মেলনে জোরালো বক্তব্য বাইডেনের</td>\n      <td>international_politics</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>35823</th>\n      <td>নিজেদের লোক দিয়েই নির্বাচন কমিশন গঠন করেছে আ. ...</td>\n      <td>bd_politics</td>\n    </tr>\n    <tr>\n      <th>35824</th>\n      <td>আমলে না নিলেও বিএনপিকে পরামর্শ দেবেন ডা. জাফরু...</td>\n      <td>bd_politics</td>\n    </tr>\n    <tr>\n      <th>35825</th>\n      <td>অগ্নিঝরা মার্চের প্রথম প্রহরে বঙ্গবন্ধুর প্রতি...</td>\n      <td>bd_politics</td>\n    </tr>\n    <tr>\n      <th>35826</th>\n      <td>দ্রব্যমূল্য নিয়ন্ত্রণে ব্যর্থ হলে সরকারকে পদত্...</td>\n      <td>bd_politics</td>\n    </tr>\n    <tr>\n      <th>35827</th>\n      <td>বিএনপি নির্বাচনের জন্য অযোগ্য হয়ে পড়েছে: ওবায়দ...</td>\n      <td>bd_politics</td>\n    </tr>\n  </tbody>\n</table>\n<p>35828 rows × 2 columns</p>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\n\n# Assuming the dataset has columns 'article' and 'tags'\nX = df['headline']\ny = df['category']\n\n# Split the dataset into training and validation sets\nX_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.3, random_state=42)\n# Further split the validation set into validation and test sets\nX_val, X_test, y_val, y_test = train_test_split(X_val, y_val, test_size=0.5, random_state=42)\n","metadata":{"execution":{"iopub.status.busy":"2024-07-11T03:28:52.965429Z","iopub.execute_input":"2024-07-11T03:28:52.966095Z","iopub.status.idle":"2024-07-11T03:28:53.504352Z","shell.execute_reply.started":"2024-07-11T03:28:52.966044Z","shell.execute_reply":"2024-07-11T03:28:53.503514Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from transformers import ElectraTokenizer, TFBertForSequenceClassification\nimport tensorflow as tf\n# Load the Bangla Electra tokenizer\ntokenizer = ElectraTokenizer.from_pretrained('csebuetnlp/banglabert')\n\n# Load the Bangla BERT model for sequence classification using PyTorch weights\nmodel = TFBertForSequenceClassification.from_pretrained('csebuetnlp/banglabert', num_labels=3, from_pt=True)\nlabels = [0, 1,2]\n\n","metadata":{"execution":{"iopub.status.busy":"2024-07-11T03:28:58.159067Z","iopub.execute_input":"2024-07-11T03:28:58.159414Z","iopub.status.idle":"2024-07-11T03:29:20.504628Z","shell.execute_reply.started":"2024-07-11T03:28:58.159381Z","shell.execute_reply":"2024-07-11T03:29:20.503748Z"},"trusted":true},"execution_count":11,"outputs":[{"name":"stderr","text":"2024-07-11 03:29:04.143051: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-07-11 03:29:04.143160: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-07-11 03:29:04.266381: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/119 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a87a4532be564c9e96e832681de418f7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt:   0%|          | 0.00/528k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f3f8f3bf044f4e59a33aaa819ce83fab"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7fee549fe969421c971af6f47c078c84"}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/586 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"405404f5381d47c09dcc299110473596"}},"metadata":{}},{"name":"stderr","text":"You are using a model of type electra to instantiate a model of type bert. This is not supported for all configurations of models and can yield errors.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"pytorch_model.bin:   0%|          | 0.00/443M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"761dfd3014454c918906ac45f61a8fe1"}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n  return self.fget.__get__(instance, owner)()\nSome weights of the PyTorch model were not used when initializing the TF 2.0 model TFBertForSequenceClassification: ['electra.encoder.layer.8.intermediate.dense.weight', 'electra.encoder.layer.5.output.LayerNorm.bias', 'electra.encoder.layer.10.attention.output.LayerNorm.weight', 'electra.encoder.layer.10.output.LayerNorm.bias', 'electra.encoder.layer.2.attention.output.dense.weight', 'electra.encoder.layer.9.intermediate.dense.weight', 'electra.encoder.layer.9.output.dense.weight', 'electra.encoder.layer.11.attention.self.value.bias', 'electra.encoder.layer.9.output.LayerNorm.bias', 'discriminator_predictions.dense_prediction.bias', 'electra.encoder.layer.10.output.dense.bias', 'electra.encoder.layer.7.attention.output.dense.weight', 'electra.encoder.layer.10.attention.self.value.weight', 'electra.encoder.layer.11.attention.self.key.bias', 'electra.encoder.layer.5.attention.output.LayerNorm.weight', 'electra.encoder.layer.8.output.dense.bias', 'electra.encoder.layer.5.output.dense.bias', 'electra.encoder.layer.11.attention.self.key.weight', 'electra.encoder.layer.7.attention.self.query.weight', 'electra.encoder.layer.3.intermediate.dense.weight', 'electra.encoder.layer.2.attention.self.key.weight', 'electra.encoder.layer.5.attention.self.query.weight', 'electra.encoder.layer.5.intermediate.dense.bias', 'electra.encoder.layer.6.attention.self.value.weight', 'discriminator_predictions.dense_prediction.weight', 'electra.encoder.layer.2.attention.output.LayerNorm.bias', 'electra.encoder.layer.2.output.LayerNorm.bias', 'electra.encoder.layer.4.output.dense.weight', 'electra.encoder.layer.4.output.dense.bias', 'electra.encoder.layer.3.attention.self.query.weight', 'electra.encoder.layer.5.intermediate.dense.weight', 'electra.encoder.layer.5.attention.self.key.bias', 'electra.encoder.layer.5.output.LayerNorm.weight', 'electra.encoder.layer.10.attention.self.query.weight', 'electra.encoder.layer.9.output.dense.bias', 'electra.encoder.layer.11.attention.self.query.bias', 'electra.encoder.layer.0.attention.self.value.bias', 'electra.encoder.layer.6.attention.self.query.bias', 'electra.encoder.layer.10.attention.self.key.weight', 'electra.encoder.layer.10.attention.self.value.bias', 'electra.encoder.layer.4.attention.output.dense.weight', 'electra.encoder.layer.11.attention.self.value.weight', 'electra.encoder.layer.7.attention.self.value.weight', 'electra.encoder.layer.9.intermediate.dense.bias', 'electra.encoder.layer.5.attention.output.dense.weight', 'electra.encoder.layer.1.output.LayerNorm.weight', 'electra.encoder.layer.2.attention.self.value.weight', 'electra.encoder.layer.0.attention.output.dense.bias', 'electra.encoder.layer.8.attention.self.key.weight', 'electra.encoder.layer.11.attention.output.LayerNorm.bias', 'electra.encoder.layer.7.output.dense.bias', 'electra.encoder.layer.9.attention.self.value.bias', 'electra.encoder.layer.6.output.LayerNorm.bias', 'electra.encoder.layer.1.attention.self.query.weight', 'electra.encoder.layer.1.attention.output.dense.weight', 'electra.encoder.layer.1.attention.output.LayerNorm.weight', 'electra.encoder.layer.9.attention.self.key.bias', 'electra.encoder.layer.8.output.LayerNorm.weight', 'electra.encoder.layer.11.attention.output.dense.weight', 'electra.encoder.layer.4.output.LayerNorm.bias', 'electra.encoder.layer.9.attention.output.dense.bias', 'electra.encoder.layer.3.attention.self.key.bias', 'electra.encoder.layer.3.output.dense.weight', 'electra.encoder.layer.9.attention.self.query.weight', 'electra.encoder.layer.3.intermediate.dense.bias', 'electra.encoder.layer.7.intermediate.dense.weight', 'electra.encoder.layer.0.attention.output.LayerNorm.bias', 'electra.encoder.layer.3.output.dense.bias', 'electra.encoder.layer.11.intermediate.dense.bias', 'electra.encoder.layer.1.intermediate.dense.bias', 'discriminator_predictions.dense.bias', 'electra.encoder.layer.0.attention.self.query.weight', 'electra.encoder.layer.3.attention.output.dense.bias', 'electra.encoder.layer.0.intermediate.dense.weight', 'electra.encoder.layer.9.attention.self.value.weight', 'electra.encoder.layer.11.output.LayerNorm.bias', 'electra.encoder.layer.7.output.LayerNorm.weight', 'electra.encoder.layer.4.attention.self.key.bias', 'electra.encoder.layer.9.attention.output.LayerNorm.bias', 'electra.encoder.layer.3.attention.self.value.weight', 'electra.encoder.layer.4.intermediate.dense.bias', 'electra.encoder.layer.4.attention.self.value.weight', 'electra.encoder.layer.7.attention.output.dense.bias', 'electra.encoder.layer.4.attention.self.query.bias', 'electra.encoder.layer.4.attention.self.value.bias', 'electra.encoder.layer.1.output.dense.bias', 'electra.encoder.layer.2.attention.output.dense.bias', 'electra.encoder.layer.6.attention.output.dense.bias', 'electra.encoder.layer.2.intermediate.dense.weight', 'electra.encoder.layer.7.output.dense.weight', 'electra.encoder.layer.1.attention.self.query.bias', 'electra.encoder.layer.6.intermediate.dense.weight', 'electra.encoder.layer.6.output.LayerNorm.weight', 'electra.encoder.layer.4.attention.output.dense.bias', 'electra.encoder.layer.10.intermediate.dense.weight', 'electra.encoder.layer.3.attention.output.LayerNorm.weight', 'electra.encoder.layer.0.attention.self.key.bias', 'electra.encoder.layer.8.attention.output.dense.weight', 'electra.encoder.layer.9.attention.output.LayerNorm.weight', 'electra.encoder.layer.8.output.LayerNorm.bias', 'electra.encoder.layer.11.intermediate.dense.weight', 'electra.encoder.layer.11.output.LayerNorm.weight', 'electra.encoder.layer.1.output.dense.weight', 'electra.encoder.layer.10.attention.output.dense.bias', 'electra.encoder.layer.0.output.dense.bias', 'electra.encoder.layer.10.intermediate.dense.bias', 'electra.encoder.layer.10.output.LayerNorm.weight', 'electra.encoder.layer.1.attention.self.value.bias', 'electra.encoder.layer.2.attention.self.value.bias', 'electra.embeddings.position_ids', 'electra.encoder.layer.5.attention.self.query.bias', 'electra.encoder.layer.5.attention.output.LayerNorm.bias', 'electra.encoder.layer.7.output.LayerNorm.bias', 'electra.encoder.layer.7.attention.output.LayerNorm.weight', 'electra.encoder.layer.3.output.LayerNorm.bias', 'electra.embeddings.LayerNorm.bias', 'electra.encoder.layer.3.attention.self.query.bias', 'electra.encoder.layer.2.intermediate.dense.bias', 'electra.encoder.layer.6.attention.self.value.bias', 'electra.encoder.layer.1.output.LayerNorm.bias', 'electra.encoder.layer.6.attention.output.LayerNorm.weight', 'electra.encoder.layer.6.attention.self.query.weight', 'electra.encoder.layer.6.attention.output.dense.weight', 'electra.encoder.layer.8.attention.self.key.bias', 'electra.encoder.layer.8.attention.output.dense.bias', 'electra.encoder.layer.11.attention.output.dense.bias', 'electra.encoder.layer.8.output.dense.weight', 'electra.encoder.layer.7.attention.self.key.weight', 'electra.encoder.layer.8.attention.output.LayerNorm.weight', 'electra.encoder.layer.2.output.dense.bias', 'electra.encoder.layer.0.intermediate.dense.bias', 'electra.encoder.layer.8.intermediate.dense.bias', 'electra.encoder.layer.0.output.LayerNorm.bias', 'electra.encoder.layer.9.attention.self.query.bias', 'electra.encoder.layer.0.attention.self.key.weight', 'electra.encoder.layer.4.attention.output.LayerNorm.weight', 'electra.encoder.layer.10.attention.self.query.bias', 'electra.encoder.layer.8.attention.self.value.bias', 'electra.encoder.layer.6.attention.self.key.weight', 'electra.encoder.layer.8.attention.self.query.bias', 'electra.embeddings.LayerNorm.weight', 'electra.encoder.layer.11.attention.output.LayerNorm.weight', 'electra.encoder.layer.0.attention.self.query.bias', 'electra.encoder.layer.6.output.dense.weight', 'electra.encoder.layer.3.attention.self.key.weight', 'electra.encoder.layer.9.attention.output.dense.weight', 'electra.encoder.layer.1.attention.output.LayerNorm.bias', 'electra.encoder.layer.0.output.dense.weight', 'electra.encoder.layer.2.output.dense.weight', 'electra.encoder.layer.8.attention.self.query.weight', 'electra.encoder.layer.5.output.dense.weight', 'electra.encoder.layer.6.attention.output.LayerNorm.bias', 'electra.encoder.layer.0.output.LayerNorm.weight', 'electra.embeddings.token_type_embeddings.weight', 'electra.encoder.layer.0.attention.output.LayerNorm.weight', 'electra.encoder.layer.1.attention.self.value.weight', 'electra.encoder.layer.7.attention.self.query.bias', 'electra.encoder.layer.4.output.LayerNorm.weight', 'electra.encoder.layer.6.output.dense.bias', 'electra.encoder.layer.1.attention.output.dense.bias', 'electra.encoder.layer.4.intermediate.dense.weight', 'electra.encoder.layer.2.attention.self.key.bias', 'electra.encoder.layer.0.attention.output.dense.weight', 'electra.encoder.layer.3.attention.output.dense.weight', 'electra.encoder.layer.4.attention.self.query.weight', 'electra.encoder.layer.1.intermediate.dense.weight', 'electra.encoder.layer.7.attention.self.value.bias', 'electra.encoder.layer.10.output.dense.weight', 'electra.encoder.layer.11.attention.self.query.weight', 'electra.encoder.layer.11.output.dense.bias', 'electra.encoder.layer.0.attention.self.value.weight', 'electra.encoder.layer.2.attention.output.LayerNorm.weight', 'electra.encoder.layer.4.attention.self.key.weight', 'electra.encoder.layer.5.attention.self.key.weight', 'electra.encoder.layer.4.attention.output.LayerNorm.bias', 'electra.encoder.layer.9.attention.self.key.weight', 'electra.encoder.layer.2.output.LayerNorm.weight', 'electra.embeddings.position_embeddings.weight', 'electra.encoder.layer.7.attention.output.LayerNorm.bias', 'electra.encoder.layer.3.attention.self.value.bias', 'electra.encoder.layer.10.attention.output.LayerNorm.bias', 'electra.encoder.layer.2.attention.self.query.bias', 'electra.encoder.layer.6.attention.self.key.bias', 'electra.embeddings.word_embeddings.weight', 'electra.encoder.layer.5.attention.output.dense.bias', 'electra.encoder.layer.7.intermediate.dense.bias', 'discriminator_predictions.dense.weight', 'electra.encoder.layer.1.attention.self.key.bias', 'electra.encoder.layer.2.attention.self.query.weight', 'electra.encoder.layer.8.attention.self.value.weight', 'electra.encoder.layer.10.attention.self.key.bias', 'electra.encoder.layer.9.output.LayerNorm.weight', 'electra.encoder.layer.10.attention.output.dense.weight', 'electra.encoder.layer.3.attention.output.LayerNorm.bias', 'electra.encoder.layer.5.attention.self.value.bias', 'electra.encoder.layer.8.attention.output.LayerNorm.bias', 'electra.encoder.layer.1.attention.self.key.weight', 'electra.encoder.layer.6.intermediate.dense.bias', 'electra.encoder.layer.7.attention.self.key.bias', 'electra.encoder.layer.3.output.LayerNorm.weight', 'electra.encoder.layer.11.output.dense.weight', 'electra.encoder.layer.5.attention.self.value.weight']\n- This IS expected if you are initializing TFBertForSequenceClassification from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing TFBertForSequenceClassification from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\nSome weights or buffers of the TF 2.0 model TFBertForSequenceClassification were not initialized from the PyTorch model and are newly initialized: ['embeddings.word_embeddings.weight', 'embeddings.token_type_embeddings.weight', 'embeddings.position_embeddings.weight', 'embeddings.LayerNorm.weight', 'embeddings.LayerNorm.bias', 'encoder.layer.0.attention.self.query.weight', 'encoder.layer.0.attention.self.query.bias', 'encoder.layer.0.attention.self.key.weight', 'encoder.layer.0.attention.self.key.bias', 'encoder.layer.0.attention.self.value.weight', 'encoder.layer.0.attention.self.value.bias', 'encoder.layer.0.attention.output.dense.weight', 'encoder.layer.0.attention.output.dense.bias', 'encoder.layer.0.attention.output.LayerNorm.weight', 'encoder.layer.0.attention.output.LayerNorm.bias', 'encoder.layer.0.intermediate.dense.weight', 'encoder.layer.0.intermediate.dense.bias', 'encoder.layer.0.output.dense.weight', 'encoder.layer.0.output.dense.bias', 'encoder.layer.0.output.LayerNorm.weight', 'encoder.layer.0.output.LayerNorm.bias', 'encoder.layer.1.attention.self.query.weight', 'encoder.layer.1.attention.self.query.bias', 'encoder.layer.1.attention.self.key.weight', 'encoder.layer.1.attention.self.key.bias', 'encoder.layer.1.attention.self.value.weight', 'encoder.layer.1.attention.self.value.bias', 'encoder.layer.1.attention.output.dense.weight', 'encoder.layer.1.attention.output.dense.bias', 'encoder.layer.1.attention.output.LayerNorm.weight', 'encoder.layer.1.attention.output.LayerNorm.bias', 'encoder.layer.1.intermediate.dense.weight', 'encoder.layer.1.intermediate.dense.bias', 'encoder.layer.1.output.dense.weight', 'encoder.layer.1.output.dense.bias', 'encoder.layer.1.output.LayerNorm.weight', 'encoder.layer.1.output.LayerNorm.bias', 'encoder.layer.2.attention.self.query.weight', 'encoder.layer.2.attention.self.query.bias', 'encoder.layer.2.attention.self.key.weight', 'encoder.layer.2.attention.self.key.bias', 'encoder.layer.2.attention.self.value.weight', 'encoder.layer.2.attention.self.value.bias', 'encoder.layer.2.attention.output.dense.weight', 'encoder.layer.2.attention.output.dense.bias', 'encoder.layer.2.attention.output.LayerNorm.weight', 'encoder.layer.2.attention.output.LayerNorm.bias', 'encoder.layer.2.intermediate.dense.weight', 'encoder.layer.2.intermediate.dense.bias', 'encoder.layer.2.output.dense.weight', 'encoder.layer.2.output.dense.bias', 'encoder.layer.2.output.LayerNorm.weight', 'encoder.layer.2.output.LayerNorm.bias', 'encoder.layer.3.attention.self.query.weight', 'encoder.layer.3.attention.self.query.bias', 'encoder.layer.3.attention.self.key.weight', 'encoder.layer.3.attention.self.key.bias', 'encoder.layer.3.attention.self.value.weight', 'encoder.layer.3.attention.self.value.bias', 'encoder.layer.3.attention.output.dense.weight', 'encoder.layer.3.attention.output.dense.bias', 'encoder.layer.3.attention.output.LayerNorm.weight', 'encoder.layer.3.attention.output.LayerNorm.bias', 'encoder.layer.3.intermediate.dense.weight', 'encoder.layer.3.intermediate.dense.bias', 'encoder.layer.3.output.dense.weight', 'encoder.layer.3.output.dense.bias', 'encoder.layer.3.output.LayerNorm.weight', 'encoder.layer.3.output.LayerNorm.bias', 'encoder.layer.4.attention.self.query.weight', 'encoder.layer.4.attention.self.query.bias', 'encoder.layer.4.attention.self.key.weight', 'encoder.layer.4.attention.self.key.bias', 'encoder.layer.4.attention.self.value.weight', 'encoder.layer.4.attention.self.value.bias', 'encoder.layer.4.attention.output.dense.weight', 'encoder.layer.4.attention.output.dense.bias', 'encoder.layer.4.attention.output.LayerNorm.weight', 'encoder.layer.4.attention.output.LayerNorm.bias', 'encoder.layer.4.intermediate.dense.weight', 'encoder.layer.4.intermediate.dense.bias', 'encoder.layer.4.output.dense.weight', 'encoder.layer.4.output.dense.bias', 'encoder.layer.4.output.LayerNorm.weight', 'encoder.layer.4.output.LayerNorm.bias', 'encoder.layer.5.attention.self.query.weight', 'encoder.layer.5.attention.self.query.bias', 'encoder.layer.5.attention.self.key.weight', 'encoder.layer.5.attention.self.key.bias', 'encoder.layer.5.attention.self.value.weight', 'encoder.layer.5.attention.self.value.bias', 'encoder.layer.5.attention.output.dense.weight', 'encoder.layer.5.attention.output.dense.bias', 'encoder.layer.5.attention.output.LayerNorm.weight', 'encoder.layer.5.attention.output.LayerNorm.bias', 'encoder.layer.5.intermediate.dense.weight', 'encoder.layer.5.intermediate.dense.bias', 'encoder.layer.5.output.dense.weight', 'encoder.layer.5.output.dense.bias', 'encoder.layer.5.output.LayerNorm.weight', 'encoder.layer.5.output.LayerNorm.bias', 'encoder.layer.6.attention.self.query.weight', 'encoder.layer.6.attention.self.query.bias', 'encoder.layer.6.attention.self.key.weight', 'encoder.layer.6.attention.self.key.bias', 'encoder.layer.6.attention.self.value.weight', 'encoder.layer.6.attention.self.value.bias', 'encoder.layer.6.attention.output.dense.weight', 'encoder.layer.6.attention.output.dense.bias', 'encoder.layer.6.attention.output.LayerNorm.weight', 'encoder.layer.6.attention.output.LayerNorm.bias', 'encoder.layer.6.intermediate.dense.weight', 'encoder.layer.6.intermediate.dense.bias', 'encoder.layer.6.output.dense.weight', 'encoder.layer.6.output.dense.bias', 'encoder.layer.6.output.LayerNorm.weight', 'encoder.layer.6.output.LayerNorm.bias', 'encoder.layer.7.attention.self.query.weight', 'encoder.layer.7.attention.self.query.bias', 'encoder.layer.7.attention.self.key.weight', 'encoder.layer.7.attention.self.key.bias', 'encoder.layer.7.attention.self.value.weight', 'encoder.layer.7.attention.self.value.bias', 'encoder.layer.7.attention.output.dense.weight', 'encoder.layer.7.attention.output.dense.bias', 'encoder.layer.7.attention.output.LayerNorm.weight', 'encoder.layer.7.attention.output.LayerNorm.bias', 'encoder.layer.7.intermediate.dense.weight', 'encoder.layer.7.intermediate.dense.bias', 'encoder.layer.7.output.dense.weight', 'encoder.layer.7.output.dense.bias', 'encoder.layer.7.output.LayerNorm.weight', 'encoder.layer.7.output.LayerNorm.bias', 'encoder.layer.8.attention.self.query.weight', 'encoder.layer.8.attention.self.query.bias', 'encoder.layer.8.attention.self.key.weight', 'encoder.layer.8.attention.self.key.bias', 'encoder.layer.8.attention.self.value.weight', 'encoder.layer.8.attention.self.value.bias', 'encoder.layer.8.attention.output.dense.weight', 'encoder.layer.8.attention.output.dense.bias', 'encoder.layer.8.attention.output.LayerNorm.weight', 'encoder.layer.8.attention.output.LayerNorm.bias', 'encoder.layer.8.intermediate.dense.weight', 'encoder.layer.8.intermediate.dense.bias', 'encoder.layer.8.output.dense.weight', 'encoder.layer.8.output.dense.bias', 'encoder.layer.8.output.LayerNorm.weight', 'encoder.layer.8.output.LayerNorm.bias', 'encoder.layer.9.attention.self.query.weight', 'encoder.layer.9.attention.self.query.bias', 'encoder.layer.9.attention.self.key.weight', 'encoder.layer.9.attention.self.key.bias', 'encoder.layer.9.attention.self.value.weight', 'encoder.layer.9.attention.self.value.bias', 'encoder.layer.9.attention.output.dense.weight', 'encoder.layer.9.attention.output.dense.bias', 'encoder.layer.9.attention.output.LayerNorm.weight', 'encoder.layer.9.attention.output.LayerNorm.bias', 'encoder.layer.9.intermediate.dense.weight', 'encoder.layer.9.intermediate.dense.bias', 'encoder.layer.9.output.dense.weight', 'encoder.layer.9.output.dense.bias', 'encoder.layer.9.output.LayerNorm.weight', 'encoder.layer.9.output.LayerNorm.bias', 'encoder.layer.10.attention.self.query.weight', 'encoder.layer.10.attention.self.query.bias', 'encoder.layer.10.attention.self.key.weight', 'encoder.layer.10.attention.self.key.bias', 'encoder.layer.10.attention.self.value.weight', 'encoder.layer.10.attention.self.value.bias', 'encoder.layer.10.attention.output.dense.weight', 'encoder.layer.10.attention.output.dense.bias', 'encoder.layer.10.attention.output.LayerNorm.weight', 'encoder.layer.10.attention.output.LayerNorm.bias', 'encoder.layer.10.intermediate.dense.weight', 'encoder.layer.10.intermediate.dense.bias', 'encoder.layer.10.output.dense.weight', 'encoder.layer.10.output.dense.bias', 'encoder.layer.10.output.LayerNorm.weight', 'encoder.layer.10.output.LayerNorm.bias', 'encoder.layer.11.attention.self.query.weight', 'encoder.layer.11.attention.self.query.bias', 'encoder.layer.11.attention.self.key.weight', 'encoder.layer.11.attention.self.key.bias', 'encoder.layer.11.attention.self.value.weight', 'encoder.layer.11.attention.self.value.bias', 'encoder.layer.11.attention.output.dense.weight', 'encoder.layer.11.attention.output.dense.bias', 'encoder.layer.11.attention.output.LayerNorm.weight', 'encoder.layer.11.attention.output.LayerNorm.bias', 'encoder.layer.11.intermediate.dense.weight', 'encoder.layer.11.intermediate.dense.bias', 'encoder.layer.11.output.dense.weight', 'encoder.layer.11.output.dense.bias', 'encoder.layer.11.output.LayerNorm.weight', 'encoder.layer.11.output.LayerNorm.bias', 'pooler.dense.weight', 'pooler.dense.bias', 'classifier.weight', 'classifier.bias']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"}]},{"cell_type":"code","source":"# Tokenize the training and validation sets\ndef tokenize_data(texts, labels, tokenizer, max_length=128):\n    input_ids = []\n    attention_masks = []\n    \n    for text in texts:\n        encoded_dict = tokenizer.encode_plus(\n            text,\n            add_special_tokens=True,\n            max_length=max_length,\n            pad_to_max_length=True,\n            return_attention_mask=True,\n            return_tensors='tf'\n        )\n        input_ids.append(encoded_dict['input_ids'])\n        attention_masks.append(encoded_dict['attention_mask'])\n\n    input_ids = tf.concat(input_ids, axis=0)\n    attention_masks = tf.concat(attention_masks, axis=0)\n    labels = tf.convert_to_tensor(labels, dtype=tf.int32)  # Convert labels to tf.int32 here\n\n    return input_ids, attention_masks, labels\n\n\n","metadata":{"execution":{"iopub.status.busy":"2024-07-11T03:29:20.506502Z","iopub.execute_input":"2024-07-11T03:29:20.506867Z","iopub.status.idle":"2024-07-11T03:29:20.516921Z","shell.execute_reply.started":"2024-07-11T03:29:20.506813Z","shell.execute_reply":"2024-07-11T03:29:20.516017Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.preprocessing import LabelEncoder\n\n# Encode labels using sklearn's LabelEncoder\nlabel_encoder = LabelEncoder()\ny_train_encoded = label_encoder.fit_transform(y_train)\ny_val_encoded = label_encoder.transform(y_val)\n\n# Now tokenize your data with encoded labels\ntrain_input_ids, train_attention_masks, train_labels = tokenize_data(X_train, y_train_encoded, tokenizer)\nval_input_ids, val_attention_masks, val_labels = tokenize_data(X_val, y_val_encoded, tokenizer)\n\n# Convert labels to TensorFlow tensors explicitly\ntrain_labels = tf.convert_to_tensor(train_labels, dtype=tf.int32)\nval_labels = tf.convert_to_tensor(val_labels, dtype=tf.int32)\n","metadata":{"execution":{"iopub.status.busy":"2024-07-11T03:29:20.518252Z","iopub.execute_input":"2024-07-11T03:29:20.518571Z","iopub.status.idle":"2024-07-11T03:29:45.170158Z","shell.execute_reply.started":"2024-07-11T03:29:20.518541Z","shell.execute_reply":"2024-07-11T03:29:45.169324Z"},"trusted":true},"execution_count":13,"outputs":[{"name":"stderr","text":"Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2699: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n  warnings.warn(\n","output_type":"stream"}]},{"cell_type":"code","source":"# Assuming you have a test dataset with columns 'headline' and 'category'\n\n# Encode labels for test data\ny_test_encoded = label_encoder.transform(y_test)\n\n# Tokenize the test data\ntest_input_ids, test_attention_masks, test_labels = tokenize_data(X_test, y_test_encoded, tokenizer)\ntest_labels = tf.convert_to_tensor(test_labels, dtype=tf.int32)","metadata":{"execution":{"iopub.status.busy":"2024-07-11T03:29:45.172611Z","iopub.execute_input":"2024-07-11T03:29:45.173255Z","iopub.status.idle":"2024-07-11T03:29:49.370566Z","shell.execute_reply.started":"2024-07-11T03:29:45.173221Z","shell.execute_reply":"2024-07-11T03:29:49.369554Z"},"trusted":true},"execution_count":14,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2699: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n  warnings.warn(\n","output_type":"stream"}]},{"cell_type":"code","source":"from transformers import TFElectraForSequenceClassification, ElectraTokenizer\nimport tensorflow as tf\n\n# Load the tokenizer and the model\nmodel = TFElectraForSequenceClassification.from_pretrained('csebuetnlp/banglabert', num_labels=3, from_pt=True)\n\n\n\n","metadata":{"execution":{"iopub.status.busy":"2024-07-11T03:29:49.371816Z","iopub.execute_input":"2024-07-11T03:29:49.372201Z","iopub.status.idle":"2024-07-11T03:29:50.969953Z","shell.execute_reply.started":"2024-07-11T03:29:49.372166Z","shell.execute_reply":"2024-07-11T03:29:50.969175Z"},"trusted":true},"execution_count":15,"outputs":[{"name":"stderr","text":"Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFElectraForSequenceClassification: ['discriminator_predictions.dense_prediction.bias', 'discriminator_predictions.dense_prediction.weight', 'discriminator_predictions.dense.bias', 'electra.embeddings.position_ids', 'discriminator_predictions.dense.weight']\n- This IS expected if you are initializing TFElectraForSequenceClassification from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing TFElectraForSequenceClassification from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\nSome weights or buffers of the TF 2.0 model TFElectraForSequenceClassification were not initialized from the PyTorch model and are newly initialized: ['classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"}]},{"cell_type":"code","source":"# Compile the model\noptimizer = tf.keras.optimizers.Adam(learning_rate=2e-5)\nloss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\nmetric = tf.keras.metrics.SparseCategoricalAccuracy('accuracy')\n\n","metadata":{"execution":{"iopub.status.busy":"2024-07-11T03:29:50.971815Z","iopub.execute_input":"2024-07-11T03:29:50.972111Z","iopub.status.idle":"2024-07-11T03:29:51.080177Z","shell.execute_reply.started":"2024-07-11T03:29:50.972086Z","shell.execute_reply":"2024-07-11T03:29:51.079280Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"model.compile(optimizer=optimizer, loss=loss, metrics=[metric])\n\n","metadata":{"execution":{"iopub.status.busy":"2024-07-11T03:29:51.081244Z","iopub.execute_input":"2024-07-11T03:29:51.081510Z","iopub.status.idle":"2024-07-11T03:29:51.095656Z","shell.execute_reply.started":"2024-07-11T03:29:51.081487Z","shell.execute_reply":"2024-07-11T03:29:51.094816Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"# Train the model\nnum_epochs = 5\nbatch_size = 64\n\nhistory = model.fit(\n    [train_input_ids, train_attention_masks], train_labels,\n    validation_data=([val_input_ids, val_attention_masks], val_labels),\n    epochs=num_epochs,\n    batch_size=batch_size\n)\n","metadata":{"execution":{"iopub.status.busy":"2024-07-11T03:29:51.097957Z","iopub.execute_input":"2024-07-11T03:29:51.098267Z","iopub.status.idle":"2024-07-11T04:02:26.606077Z","shell.execute_reply.started":"2024-07-11T03:29:51.098243Z","shell.execute_reply":"2024-07-11T04:02:26.605231Z"},"trusted":true},"execution_count":18,"outputs":[{"name":"stdout","text":"Epoch 1/5\nWARNING: AutoGraph could not transform <function infer_framework at 0x7f7ef7d98310> and will run it as-is.\nCause: for/else statement not yet supported\nTo silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n","output_type":"stream"},{"name":"stderr","text":"WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nI0000 00:00:1720668666.940654     122 device_compiler.h:186] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n","output_type":"stream"},{"name":"stdout","text":"392/392 [==============================] - 473s 980ms/step - loss: 0.3603 - accuracy: 0.8714 - val_loss: 0.2213 - val_accuracy: 0.9259\nEpoch 2/5\n392/392 [==============================] - 371s 945ms/step - loss: 0.1749 - accuracy: 0.9427 - val_loss: 0.2235 - val_accuracy: 0.9282\nEpoch 3/5\n392/392 [==============================] - 371s 945ms/step - loss: 0.1260 - accuracy: 0.9598 - val_loss: 0.2205 - val_accuracy: 0.9341\nEpoch 4/5\n392/392 [==============================] - 371s 946ms/step - loss: 0.0851 - accuracy: 0.9744 - val_loss: 0.2348 - val_accuracy: 0.9365\nEpoch 5/5\n392/392 [==============================] - 371s 946ms/step - loss: 0.0619 - accuracy: 0.9810 - val_loss: 0.2735 - val_accuracy: 0.9310\n","output_type":"stream"}]},{"cell_type":"code","source":"# Save the fine-tuned model\nmodel.save_pretrained('./fine_tuned_banglabert')\n\n# Evaluate the model\nresults = model.evaluate([val_input_ids, val_attention_masks], val_labels)\nprint(f\"Validation Loss: {results[0]}\")\nprint(f\"Validation Accuracy: {results[1]}\")\n","metadata":{"execution":{"iopub.status.busy":"2024-07-11T04:02:26.608343Z","iopub.execute_input":"2024-07-11T04:02:26.608779Z","iopub.status.idle":"2024-07-11T04:02:55.690203Z","shell.execute_reply.started":"2024-07-11T04:02:26.608746Z","shell.execute_reply":"2024-07-11T04:02:55.689259Z"},"trusted":true},"execution_count":19,"outputs":[{"name":"stdout","text":"168/168 [==============================] - 28s 165ms/step - loss: 0.2735 - accuracy: 0.9310\nValidation Loss: 0.27352628111839294\nValidation Accuracy: 0.9309638738632202\n","output_type":"stream"}]},{"cell_type":"code","source":"# Evaluate the model on the test data\nresults = model.evaluate([test_input_ids, test_attention_masks], test_labels)\nprint(\"Test accuracy:\", results[1])  # Assuming index 1 contains accuracy metric","metadata":{"execution":{"iopub.status.busy":"2024-07-11T04:02:55.691362Z","iopub.execute_input":"2024-07-11T04:02:55.691658Z","iopub.status.idle":"2024-07-11T04:03:23.561747Z","shell.execute_reply.started":"2024-07-11T04:02:55.691632Z","shell.execute_reply":"2024-07-11T04:03:23.560804Z"},"trusted":true},"execution_count":20,"outputs":[{"name":"stdout","text":"168/168 [==============================] - 28s 165ms/step - loss: 0.2703 - accuracy: 0.9276\nTest accuracy: 0.9276279211044312\n","output_type":"stream"}]},{"cell_type":"code","source":"from sklearn.metrics import confusion_matrix, classification_report\n# Make predictions\npredictions = model.predict([test_input_ids, test_attention_masks])\npredicted_labels = tf.argmax(predictions.logits, axis=1).numpy()\n\n# Encode labels for comparison\nlabel_encoder = LabelEncoder()\nlabel_encoder.fit(y_test)\ny_test_encoded = label_encoder.transform(y_test)\nclasses = label_encoder.classes_\n\n# Create confusion matrix for 'bd_politics' and 'neutral' tags\ntarget_names = ['bd_politics', 'non_politics','international_politics']\nconf_matrix = confusion_matrix(y_test_encoded, predicted_labels, labels=label_encoder.transform(target_names))\n\n# Display confusion matrix\nprint(\"Confusion Matrix:\")\nprint(pd.DataFrame(conf_matrix, index=target_names, columns=target_names))\n\n# Optionally, you can print a classification report for more detailed metrics\nprint(\"\\nClassification Report:\")\nprint(classification_report(y_test_encoded, predicted_labels, target_names=classes))","metadata":{"execution":{"iopub.status.busy":"2024-07-11T04:03:23.563644Z","iopub.execute_input":"2024-07-11T04:03:23.563968Z","iopub.status.idle":"2024-07-11T04:04:00.494730Z","shell.execute_reply.started":"2024-07-11T04:03:23.563943Z","shell.execute_reply":"2024-07-11T04:04:00.493816Z"},"trusted":true},"execution_count":21,"outputs":[{"name":"stdout","text":"168/168 [==============================] - 37s 162ms/step\nConfusion Matrix:\n                        bd_politics  non_politics  international_politics\nbd_politics                    1802            65                      11\nnon_politics                    160          1937                      73\ninternational_politics           15            65                    1247\n\nClassification Report:\n                        precision    recall  f1-score   support\n\n           bd_politics       0.91      0.96      0.93      1878\ninternational_politics       0.94      0.94      0.94      1327\n          non_politics       0.94      0.89      0.91      2170\n\n              accuracy                           0.93      5375\n             macro avg       0.93      0.93      0.93      5375\n          weighted avg       0.93      0.93      0.93      5375\n\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\n# Example of a random Bengali sentence\nrandom_sentence = \"আমি সিয়াম , মেশিন লার্নিং করি  \"\n\n# Tokenize the random sentence\nencoded_dict = tokenizer.encode_plus(\n    random_sentence,\n    add_special_tokens=True,\n    max_length=128,\n    padding='max_length',\n    return_attention_mask=True,\n    return_tensors='tf'\n)\n\n# Extract input_ids and attention_mask from the encoded dictionary\ninput_ids = encoded_dict['input_ids']\nattention_mask = encoded_dict['attention_mask']\n\n# Make predictions\npredictions = model.predict([input_ids, attention_mask])\n\n# Convert predictions to labels\npredicted_labels = np.argmax(predictions.logits, axis=1)\npredicted_label = label_encoder.inverse_transform(predicted_labels)[0]\n\nprint(f\"Random Sentence: {random_sentence}\")\nprint(f\"Predicted Tag: {predicted_label}\")\n","metadata":{"execution":{"iopub.status.busy":"2024-07-11T04:04:00.495940Z","iopub.execute_input":"2024-07-11T04:04:00.496228Z","iopub.status.idle":"2024-07-11T04:04:00.595483Z","shell.execute_reply.started":"2024-07-11T04:04:00.496204Z","shell.execute_reply":"2024-07-11T04:04:00.594633Z"},"trusted":true},"execution_count":22,"outputs":[{"name":"stdout","text":"1/1 [==============================] - 0s 42ms/step\nRandom Sentence: আমি সিয়াম , মেশিন লার্নিং করি  \nPredicted Tag: non_politics\n","output_type":"stream"}]},{"cell_type":"code","source":"import numpy as np\n# Example of a random Bengali sentence\nrandom_sentence = \"আমি শেখ হাসিনা  , আওয়ামী লীগ করি\"\n\n# Tokenize the random sentence\nencoded_dict = tokenizer.encode_plus(\n    random_sentence,\n    add_special_tokens=True,\n    max_length=128,\n    padding='max_length',\n    return_attention_mask=True,\n    return_tensors='tf'\n)\n\n# Extract input_ids and attention_mask from the encoded dictionary\ninput_ids = encoded_dict['input_ids']\nattention_mask = encoded_dict['attention_mask']\n\n# Make predictions\npredictions = model.predict([input_ids, attention_mask])\n\n# Convert predictions to labels\npredicted_labels = np.argmax(predictions.logits, axis=1)\npredicted_label = label_encoder.inverse_transform(predicted_labels)[0]\n\nprint(f\"Random Sentence: {random_sentence}\")\nprint(f\"Predicted Tag: {predicted_label}\")\n","metadata":{"execution":{"iopub.status.busy":"2024-07-11T04:04:00.596518Z","iopub.execute_input":"2024-07-11T04:04:00.596802Z","iopub.status.idle":"2024-07-11T04:04:00.693667Z","shell.execute_reply.started":"2024-07-11T04:04:00.596776Z","shell.execute_reply":"2024-07-11T04:04:00.692818Z"},"trusted":true},"execution_count":23,"outputs":[{"name":"stdout","text":"1/1 [==============================] - 0s 42ms/step\nRandom Sentence: আমি শেখ হাসিনা  , আওয়ামী লীগ করি\nPredicted Tag: bd_politics\n","output_type":"stream"}]},{"cell_type":"code","source":"import numpy as np\n# Example of a random Bengali sentence\nrandom_sentence = \"আমি জো বাইডেন   , আমেরিকায় থাকি  \"\n\n# Tokenize the random sentence\nencoded_dict = tokenizer.encode_plus(\n    random_sentence,\n    add_special_tokens=True,\n    max_length=128,\n    padding='max_length',\n    return_attention_mask=True,\n    return_tensors='tf'\n)\n\n# Extract input_ids and attention_mask from the encoded dictionary\ninput_ids = encoded_dict['input_ids']\nattention_mask = encoded_dict['attention_mask']\n\n# Make predictions\npredictions = model.predict([input_ids, attention_mask])\n\n# Convert predictions to labels\npredicted_labels = np.argmax(predictions.logits, axis=1)\npredicted_label = label_encoder.inverse_transform(predicted_labels)[0]\n\nprint(f\"Random Sentence: {random_sentence}\")\nprint(f\"Predicted Tag: {predicted_label}\")\n","metadata":{"execution":{"iopub.status.busy":"2024-07-11T04:16:19.068390Z","iopub.execute_input":"2024-07-11T04:16:19.068747Z","iopub.status.idle":"2024-07-11T04:16:19.164001Z","shell.execute_reply.started":"2024-07-11T04:16:19.068718Z","shell.execute_reply":"2024-07-11T04:16:19.163118Z"},"trusted":true},"execution_count":26,"outputs":[{"name":"stdout","text":"1/1 [==============================] - 0s 40ms/step\nRandom Sentence: আমি জো বাইডেন   , আমেরিকায় থাকি  \nPredicted Tag: international_politics\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}